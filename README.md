# Zero-Shot Braille-to-Speech System

> Real-time assistive technology using machine learning for accessible reading

[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**Live Local Demo:** Run `python inference/unified_server.py` and open `live_demo.html` in browser

---

## ðŸŽ¯ Project Overview

An end-to-end machine learning system that enables real-time character recognition through two innovative modes:

- **Finger Reading Mode:** Recognizes characters from hand movements using contrastive learning (87% accuracy)
- **Braille Recognition Mode:** Detects and translates Braille text using CNN (98.7% accuracy)

Built for accessibility, this system helps visually impaired users by converting visual input to audio output in real-time.

---

## âœ¨ Key Features

- âœ… **Dual Recognition Modes:** Finger reading + Braille OCR
- âœ… Self-Supervised Learning: Contrastive learning without labeled data
- âœ… **Real-Time Inference:** <200ms latency via WebSocket
- âœ… **Zero-Shot Learning:** Contrastive learning with FAISS similarity search
- âœ… **Web-Based Interface:** Works on desktop and mobile browsers
- âœ… **Text-to-Speech:** Automatic audio feedback
- âœ… **High Accuracy:** 87% (finger) and 98.7% (Braille)

---

## ðŸš€ Quick Start

### **Prerequisites**

- Python 3.10+
- Webcam
- Modern web browser (Chrome/Edge/Firefox/Brave)

### **Installation**

```bash
# Clone repository
git clone https://github.com/ljunior23/zero-shot-braille-to-speech.git
cd zero-shot-braille-to-speech

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### **Run Local Demo**

```bash
# Start the backend server
python inference/unified_server.py

# Output:
# ðŸš€ Starting Unified Inference Server
# âœ“ Finger reading model loaded
# âœ“ Loaded index with 1268 embeddings
# âœ… Server ready!
# ðŸŒ Starting WebSocket server on ws://0.0.0.0:8765
```

**Then open in browser:**

Option 1: Open file directly
```
file:///path/to/inference/live_demo.html
```

Option 2: Use Python HTTP server
```bash
cd inference
python -m http.server 8000
# Open: http://localhost:8000/live_demo.html
```

---

## ðŸŽ® Usage Guide

### **Finger Reading Mode**

1. Click **"Finger Reading"** button
2. Allow camera access
3. Show your **index finger** to camera
4. Move finger to "write" characters in the air
5. System recognizes character and speaks it aloud

**Tips:**
- Keep finger steady for 2-3 seconds per character
- Use clear, distinct movements
- Works best with good lighting

### **Braille Mode**

1. Click **"Braille"** button  
2. Allow camera access
3. Point camera at Braille text
4. Place **index finger** near the cell you want to read
5. System detects Braille and speaks the character

---

## ðŸ“Š Model Performance

### **Finger Reading Model**

| Metric | Value |
|--------|-------|
| **Architecture** | Contrastive Learning + FAISS |
| **Embedding Dimension** | 128 |
| **Training Samples** | ~1,268 triplets |
| **Validation Accuracy** | 87% |
| **Inference Time** | <200ms |

### **Braille Recognition Model**

| Metric | Value |
|--------|-------|
| **Architecture** | 3-Layer CNN |
| **Parameters** | 2.8M |
| **Training Dataset** | 26,000 synthetic images |
| **Validation Accuracy** | 98.7% |
| **Inference Time** | <10ms |

---

## ðŸ“ Project Structure

```
zero-shot-braille-to-speech/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â”œâ”€â”€ training/                          # Model training
â”‚   â”œâ”€â”€ model.py                       # ContrastiveModel architecture
â”‚   â””â”€â”€ train.py                       # Training script
|
â”œâ”€â”€ preprocessing/                     # Data preprocessing
â”‚   â”œâ”€â”€ align_triplets.py              # Create contrastive triplets
â”‚   â”œâ”€â”€ extract_finger_motion.py       # Process hand tracking
â”‚   â”œâ”€â”€ extract_audio_embeddings.py    # HuBERT embeddings
â”‚   â””â”€â”€ process_imu.py                 # IMU sensor data
|
â”œâ”€â”€ data_collection/                   # Data collection tools
â”‚   â”œâ”€â”€ download_text.py               # Download text corpus
â”‚   â”œâ”€â”€ generate_tts.py                # Generate TTS audio
â”‚   â”œâ”€â”€ record_webcam.html             # Webcam recording
â”‚   â””â”€â”€ record_imu.html                # IMU recording
|
â”œâ”€â”€ braille/                           # Braille recognition
â”‚   â”œâ”€â”€ generate_training_data.py      # Generate 26K images
â”‚   â””â”€â”€ train_braille_cnn.py           # Train CNN
â”‚
â”œâ”€â”€ inference/                         # Production inference
â”‚   â”œâ”€â”€ unified_server.py              # â­ Main WebSocket server
â”‚   â”œâ”€â”€ live_demo.html                 # â­ Web UI
â”‚   â”œâ”€â”€ build_inference_index.py       # Build FAISS index
â”‚   â””â”€â”€ nearest_neighbor.py            # K-NN search
â”‚
â”œâ”€â”€ models/                            # Trained models
â”‚   â”œâ”€â”€ best_model.pt                  # Finger reading (50MB)
â”‚   â”œâ”€â”€ inference_index.pkl            # FAISS index (10MB)
â”‚   â””â”€â”€ braille_cnn.pt                 # Braille CNN (11MB)
â”‚
â””â”€â”€ data/                              # Training data
    â””â”€â”€ braille_dataset/               # 26K images
```

---

## ðŸ› ï¸ Training Models from Scratch

### **Finger Reading Model**

```bash
# Collect & preprocess data
python data_collection/download_text.py
python preprocessing/align_triplets.py

# Train model
python training/train.py

# Build inference index
python inference/build_inference_index.py

```
### **Key Technique: Self-Supervised Contrastive Learning**

- No labeled data required - learns from similarity relationships
- Triplet loss (anchor, positive, negative) for discriminative embeddings
- NT-Xent loss pushes similar samples together, different samples apart
- Results in robust 128D embeddings for zero-shot recognition


### **Braille Recognition Model**

```bash
# Generate synthetic dataset
python braille/generate_training_data.py

# Train CNN
python braille/train_braille_cnn.py
```

---

## ðŸ”¬ Technical Concepts Implemented

**Machine Learning:**

- Self-supervised learning (contrastive approach)
- Convolutional Neural Networks (CNNs)
- Embedding spaces and similarity search
- Zero-shot recognition via K-NN
- Triplet/contrastive loss functions

**Computer Vision:**

- Hand landmark tracking (MediaPipe)
- Image preprocessing and thresholding
- Contour detection and filtering
- Spatial clustering algorithms

**Systems & Infrastructure:**

- WebSocket real-time communication
- Asynchronous Python (asyncio)
- Client-server architecture
- FAISS for efficient similarity search

## ðŸ“¦ Dependencies

```txt
torch==2.0.1              # Deep learning
opencv-python==4.8.0.74   # Computer vision
mediapipe==0.10.0         # Hand tracking
websockets==11.0.3        # Real-time communication
faiss-cpu==1.7.4          # Similarity search
numpy==1.24.3             # Numerical computing
```

**Full list:** See `requirements.txt`

---

## ðŸš§ Future Work

### **Production Deployment** (Next Priority)

- [ ] Docker containerization
- [ ] AWS EC2 deployment with HTTPS
- [ ] SSL certificate configuration
- [ ] Domain name setup
- [ ] Health monitoring & logging

**Status:** Local demo complete, deployment infrastructure prepared  
**Blocker:** WebSocket routing in containerized environment  
**ETA:** 2-3 days for full resolution

### **Model Improvements**

- [ ] Expand character set (lowercase, special chars)
- [ ] Multi-language support
- [ ] Improve similar-shape accuracy (O/Q, I/J)
- [ ] Add confidence scores

### **Feature Enhancements**

- [ ] Multi-hand tracking
- [ ] Offline mode
- [ ] Mobile app (React Native)
- [ ] Voice commands

---

## ðŸ“„ License

MIT License

---

## ðŸŽ¯ For Recruiters

**This project demonstrates:**

âœ… **End-to-End ML Pipeline:** Data generation â†’ Training â†’ Inference  
âœ… **Computer Vision:** Hand tracking, object detection, pattern recognition  
âœ… **Deep Learning:** PyTorch, CNNs, contrastive learning  
âœ… **Real-Time Systems:** WebSocket, async processing, <200ms latency  
âœ… **Software Engineering:** Clean code, documentation, testing  
âœ… **Problem Solving:** Self-directed research and implementation  

**Live Demo:** Functional locally, can demonstrate during interview  
**Code Quality:** Well-documented, modular, production-ready  
**Impact:** Assistive technology for accessibility  

---

## ðŸ“ž Contact

**George Kumi Acheampong**  
ðŸ“§ kwameleon21@gmail.com  
ðŸ’¼ [LinkedIn](http://linkedin.com/in/george-acheampong-604a821b5)  
ðŸ’» [GitHub](https://github.com/ljunior23)  

---

## ðŸ“Š Project Stats

```
Development Time:      80+ hours
Lines of Code:         6,500+
Training Data:         26,000+ images
Model Accuracy:        87% & 98.7%
Inference Latency:     <200ms
Status:                âœ… Local demo complete
```

---

*Built for accessibility and education â€¢ December 2025*